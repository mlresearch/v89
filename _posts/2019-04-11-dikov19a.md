---
title: Bayesian Learning of Neural Network Architectures
abstract: In this paper we propose a Bayesian method for estimating architectural
  parameters of neural networks, namely layer size and network depth. We do this by
  learning concrete distributions over these parameters. Our results show that regular
  networks with a learned structure can generalise better on small datasets, while
  fully stochastic networks can be more robust to parameter initialisation. The proposed
  method relies on standard neural variational learning and, unlike randomised architecture
  search, does not require a retraining of the model, thus keeping the computational
  overhead at minimum.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: dikov19a
month: 0
tex_title: Bayesian Learning of Neural Network Architectures
firstpage: 730
lastpage: 738
page: 730-738
order: 730
cycles: false
bibtex_author: Dikov, Georgi and Bayer, Justin
author:
- given: Georgi
  family: Dikov
- given: Justin
  family: Bayer
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/dikov19a/dikov19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
