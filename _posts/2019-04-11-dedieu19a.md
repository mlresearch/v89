---
title: Error bounds for sparse classifiers in high-dimensions
abstract: 'We prove an L2 recovery bound for a family of sparse estimators defined
  as minimizers of some empirical loss functions â€“ which include hinge loss and logistic
  loss. More precisely, we achieve an upper-bound for coefficients estimation scaling
  as $(k\ast/n)\log(p/k\ast)$: n,p is the size of the design matrix and k* the dimension
  of the theoretical loss minimizer. This is done under standard assumptions, for
  which we derive stronger versions of a cone condition and a restricted strong convexity.
  Our bound holds with high probability and in expectation and applies to an L1-regularized
  estimator and to a recently introduced Slope estimator, which we generalize for
  classification problems.  Slope presents the advantage of adapting to unknown sparsity.
  Thus, we propose a tractable proximal algorithm to compute it and assess its empirical
  performance. Our results match the best existing bounds for classification and regression
  problems.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: dedieu19a
month: 0
tex_title: Error bounds for sparse classifiers in high-dimensions
firstpage: 48
lastpage: 56
page: 48-56
order: 48
cycles: false
bibtex_author: Dedieu, Antoine
author:
- given: Antoine
  family: Dedieu
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/dedieu19a/dedieu19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/dedieu19a/dedieu19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
