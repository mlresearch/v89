---
title: Generalizing the theory of cooperative inference
abstract: Cooperation information sharing is important to theories of human learning
  and has potential implications for machine learning. Prior work derived conditions
  for achieving optimal Cooperative Inference given strong, relatively restrictive
  assumptions. We relax these assumptions by demonstrating convergence for any discrete
  joint distribution, robustness through equivalence classes and stability under perturbation,
  and effectiveness by deriving bounds from structural properties of the original
  joint distribution. We provide geometric interpretations, connections to and implications
  for optimal transport, and connections to importance sampling, and  conclude by
  outlining open questions and challenges to realizing the promise of Cooperative
  Inference.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wang19c
month: 0
tex_title: Generalizing the theory of cooperative inference
firstpage: 1841
lastpage: 1850
page: 1841-1850
order: 1841
cycles: false
bibtex_author: Wang, Pei and Paranamana, Pushpi and Shafto, Patrick
author:
- given: Pei
  family: Wang
- given: Pushpi
  family: Paranamana
- given: Patrick
  family: Shafto
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/wang19c/wang19c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/wang19c/wang19c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
