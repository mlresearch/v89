---
title: Towards Gradient Free and Projection Free Stochastic Optimization
abstract: This paper focuses on the problem of \emph{constrained} \emph{stochastic}
  optimization. A zeroth order Frank-Wolfe algorithm is proposed, which in addition
  to the projection-free nature of the vanilla Frank-Wolfe algorithm makes it gradient
  free. Under convexity and smoothness assumption, we show that the proposed algorithm
  converges to the optimal objective function at a rate $O\left(1/T^{1/3}\right)$,
  where $T$ denotes the iteration count. In particular, the primal sub-optimality
  gap is shown to have a dimension dependence of $O\left(d^{1/3}\right)$, which is
  the best known dimension dependence among all zeroth order optimization algorithms
  with one directional derivative per iteration. For non-convex functions, we obtain
  the \emph{Frank-Wolfe} gap to be $O\left(d^{1/3}T^{-1/4}\right)$. Experiments on
  black-box optimization setups demonstrate the efficacy of the proposed algorithm.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: sahu19a
month: 0
tex_title: Towards Gradient Free and Projection Free Stochastic Optimization
firstpage: 3468
lastpage: 3477
page: 3468-3477
order: 3468
cycles: false
bibtex_author: Sahu, Anit Kumar and Zaheer, Manzil and Kar, Soummya
author:
- given: Anit Kumar
  family: Sahu
- given: Manzil
  family: Zaheer
- given: Soummya
  family: Kar
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of Machine Learning Research
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/sahu19a/sahu19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/sahu19a/sahu19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
