---
title: Restarting Frank-Wolfe
abstract: Conditional Gradients (aka Frank-Wolfe algorithms) form a classical set
  of methods for constrained smooth convex minimization due to their simplicity, the
  absence of projection step, and competitive numerical performance. While the vanilla
  Frank-Wolfe algorithm only ensures a worst-case rate of $O(1/\epsilon)$, various
  recent results  have shown that for strongly convex functions, the method can be
  slightly modified to achieve linear convergence. However, this still leaves a huge
  gap between sublinear $O(1/\epsilon)$ convergence and linear $O(\log 1/\epsilon)$
  convergence to reach an $\epsilon$-approximate solution. Here, we present a new
  variant of Conditional Gradients, that can dynamically adapt to the function’s geometric
  properties using restarts and thus smoothly interpolates between the sublinear and
  linear regimes. Furthermore, our results apply to generic compact convex constraint
  sets.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kerdreux19a
month: 0
tex_title: Restarting Frank-Wolfe
firstpage: 1275
lastpage: 1283
page: 1275-1283
order: 1275
cycles: false
bibtex_author: Kerdreux, Thomas and d'Aspremont, Alexandre and Pokutta, Sebastian
author:
- given: Thomas
  family: Kerdreux
- given: Alexandre
  family: d’Aspremont
- given: Sebastian
  family: Pokutta
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/kerdreux19a/kerdreux19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/kerdreux19a/kerdreux19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
