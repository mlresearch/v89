---
title: Exploring Fast and Communication-Efficient Algorithms in Large-Scale Distributed
  Networks
abstract: The communication overhead  has become a significant bottleneck in data-parallel
  network with the increasing of model size and data samples. In this work, we propose
  a new algorithm LPC-SVRG with quantized gradients and its acceleration ALPC-SVRG
  to effectively reduce the communication complexity while maintaining the same convergence
  as the unquantized algorithms. Specifically, we formulate the heuristic gradient
  clipping technique within the quantization scheme and show that unbiased quantization
  methods in related works [3, 33, 38] are special cases of ours. We introduce double
  sampling in the accelerated algorithm ALPC-SVRG to fully combine the gradients of
  full-precision and low-precision, and then achieve acceleration with fewer communication
  overhead. Our analysis focuses on the nonsmooth composite problem, which makes our
  algorithms more general. The experiments on linear models and deep neural networks
  validate the effectiveness of our algorithms.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yu19a
month: 0
tex_title: Exploring Fast and Communication-Efficient Algorithms in Large-Scale Distributed
  Networks
firstpage: 674
lastpage: 683
page: 674-683
order: 674
cycles: false
bibtex_author: Yu, Yue and Wu, Jiaxiang and Huang, Junzhou
author:
- given: Yue
  family: Yu
- given: Jiaxiang
  family: Wu
- given: Junzhou
  family: Huang
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of Machine Learning Research
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/yu19a/yu19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/yu19a/yu19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
