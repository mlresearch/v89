---
title: Deep Neural Networks with Multi-Branch Architectures Are Intrinsically Less
  Non-Convex
abstract: 'Several recently proposed architectures of neural networks such as ResNeXt,
  Inception, Xception, SqueezeNet and Wide ResNet are based on the designing idea
  of having multiple branches and have demonstrated improved performance in many applications.
  We show that one cause for such success is due to the fact that the multi-branch
  architecture is less non-convex in terms of duality gap. The duality gap measures
  the degree of intrinsic non-convexity of an optimization problem: smaller gap in
  relative value implies lower degree of intrinsic non-convexity. The challenge is
  to quantitatively measure the duality gap of highly non-convex problems such as
  deep neural networks. In this work, we provide strong guarantees of this quantity
  for two classes of network architectures. For the neural networks with arbitrary
  activation functions, multi-branch architecture and a variant of hinge loss, we
  show that the duality gap of both population and empirical risks shrinks to zero
  as the number of branches increases. This result sheds light on better understanding
  the power of over-parametrization where increasing the number of branches tends
  to make the loss surface less non-convex. For the neural networks with linear activation
  function and $\ell_2$ loss, we show that the duality gap of empirical risk is zero.
  Our two results work for arbitrary depths, while the analytical techniques might
  be of independent interest to non-convex optimization more broadly. Experiments
  on both synthetic and real-world datasets validate our results.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhang19d
month: 0
tex_title: Deep Neural Networks with Multi-Branch Architectures Are Intrinsically
  Less Non-Convex
firstpage: 1099
lastpage: 1109
page: 1099-1109
order: 1099
cycles: false
bibtex_author: Zhang, Hongyang and Shao, Junru and Salakhutdinov, Ruslan
author:
- given: Hongyang
  family: Zhang
- given: Junru
  family: Shao
- given: Ruslan
  family: Salakhutdinov
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/zhang19d/zhang19d.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/zhang19d/zhang19d-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
