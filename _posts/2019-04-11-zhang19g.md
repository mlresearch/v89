---
title: Learning One-hidden-layer ReLU Networks via Gradient Descent
abstract: We study the problem of learning one-hidden-layer neural networks with Rectified
  Linear Unit (ReLU) activation function, where the inputs are sampled from standard
  Gaussian distribution and the outputs are generated from a noisy teacher network.
  We analyze the performance of gradient descent for training such kind of neural
  networks based on empirical risk minimization, and provide algorithm-dependent guarantees.
  In particular, we prove that tensor initialization followed by gradient descent
  can converge to the ground-truth parameters at a linear rate up to some statistical
  error. To the best of our knowledge, this is the first work characterizing the recovery
  guarantee for practical learning of one-hidden-layer ReLU networks with multiple
  neurons. Numerical experiments verify our theoretical findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhang19g
month: 0
tex_title: Learning One-hidden-layer ReLU Networks via Gradient Descent
firstpage: 1524
lastpage: 1534
page: 1524-1534
order: 1524
cycles: false
bibtex_author: Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan
author:
- given: Xiao
  family: Zhang
- given: Yaodong
  family: Yu
- given: Lingxiao
  family: Wang
- given: Quanquan
  family: Gu
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/zhang19g/zhang19g.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
