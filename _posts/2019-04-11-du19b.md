---
title: Linear Convergence of the Primal-Dual Gradient Method for Convex-Concave Saddle
  Point Problems without Strong Convexity
abstract: We consider the convex-concave saddle point problem $\min_{x}\max_{y} f(x)+y^\top
  A x-g(y)$ where $f$ is smooth and convex and $g$ is smooth and strongly convex.
  We prove that if the coupling matrix $A$ has full column rank, the vanilla primal-dual
  gradient method can achieve linear convergence even if $f$ is not strongly convex.
  Our result generalizes previous work which either requires $f$ and $g$ to be quadratic
  functions or requires proximal mappings for both $f$ and $g$. We adopt a novel analysis
  technique that in each iteration uses a "ghost" update as a reference, and show
  that the iterates in the primal-dual gradient method converge to this "ghost" sequence.
  Using the same technique we further give an analysis for the primal-dual stochastic
  variance reduced gradient method for convex-concave saddle point problems with a
  finite-sum structure.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: du19b
month: 0
tex_title: Linear Convergence of the Primal-Dual Gradient Method for Convex-Concave
  Saddle Point Problems without Strong Convexity
firstpage: 196
lastpage: 205
page: 196-205
order: 196
cycles: false
bibtex_author: Du, Simon S. and Hu, Wei
author:
- given: Simon S.
  family: Du
- given: Wei
  family: Hu
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of Machine Learning Research
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/du19b/du19b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/du19b/du19b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
