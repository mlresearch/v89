---
title: Computation Efficient Coded Linear Transform
abstract: In large-scale distributed linear transform problems, coded computation
  plays an important role to reduce the delay caused by slow machines. However, existing
  coded schemes could end up destroying the significant sparsity that exists in large-scale
  machine learning problems, and in turn increase the computational delay. In this
  paper, we propose a coded computation strategy, referred to as diagonal code, that
  achieves the optimum recovery threshold and the optimum computation load. Furthermore,
  by leveraging the ideas from random proposal graph theory, we design a random code
  that achieves a constant computation load, which significantly outperforms the existing
  best known result. We apply our schemes to the distributed gradient descent problem
  and demonstrate the advantage of the approach over current fastest coded schemes.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wang19a
month: 0
tex_title: Computation Efficient Coded Linear Transform
firstpage: 577
lastpage: 585
page: 577-585
order: 577
cycles: false
bibtex_author: Wang, Sinong and Liu, Jiashang and Shroff, Ness and Yang, Pengyu
author:
- given: Sinong
  family: Wang
- given: Jiashang
  family: Liu
- given: Ness
  family: Shroff
- given: Pengyu
  family: Yang
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/wang19a/wang19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/wang19a/wang19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
