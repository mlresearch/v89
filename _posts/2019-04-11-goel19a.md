---
title: An Online Algorithm for Smoothed Regression and LQR Control
abstract: We consider Online Convex Optimization (OCO) in the setting where the costs
  are $m$-strongly convex and the online learner pays a switching cost for changing
  decisions between rounds. We show that the recently proposed Online Balanced Descent
  (OBD) algorithm  is constant competitive in this setting, with competitive ratio
  $3 + O(1/m)$, irrespective of the ambient dimension.  Additionally, we show that
  when the sequence of cost functions is $\epsilon$-smooth, OBD has near-optimal dynamic
  regret and maintains strong per-round accuracy. We demonstrate the generality of
  our approach by showing that the OBD framework can be used to construct competitive
  algorithms for a variety of online problems across learning and control, including
  online variants of ridge regression, logistic regression, maximum likelihood estimation,
  and LQR control.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: goel19a
month: 0
tex_title: An Online Algorithm for Smoothed Regression and LQR Control
firstpage: 2504
lastpage: 2513
page: 2504-2513
order: 2504
cycles: false
bibtex_author: Goel, Gautam and Wierman, Adam
author:
- given: Gautam
  family: Goel
- given: Adam
  family: Wierman
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/goel19a/goel19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
