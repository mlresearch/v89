---
title: On the Dynamics of Gradient Descent for Autoencoders
abstract: 'We provide a series of results for unsupervised learning with autoencoders.  Specifically,
  we study shallow two-layer autoencoder architectures with shared weights. We focus
  on three generative models for data that are common in statistical machine learning:
  (i) the mixture-of-gaussians model, (ii) the sparse coding model, and (iii) the
  sparsity model with non-negative coefficients. For each of these models, we prove
  that under suitable choices of hyperparameters, architectures, and initialization,
  autoencoders learned by gradient descent can successfully recover the parameters
  of the corresponding model. To our knowledge, this is the first result that rigorously
  studies the dynamics of gradient descent for weight-sharing autoencoders. Our analysis
  can be viewed as theoretical evidence that shallow autoencoder modules indeed can
  be used as feature learning mechanisms for a variety of data models, and may shed
  insight on how to train larger stacked architectures with autoencoders as basic
  building blocks.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: nguyen19a
month: 0
tex_title: On the Dynamics of Gradient Descent for Autoencoders
firstpage: 2858
lastpage: 2867
page: 2858-2867
order: 2858
cycles: false
bibtex_author: Nguyen, Thanh V. and Wong, Raymond K. W. and Hegde, Chinmay
author:
- given: Thanh V.
  family: Nguyen
- given: Raymond K. W.
  family: Wong
- given: Chinmay
  family: Hegde
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/nguyen19a/nguyen19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/nguyen19a/nguyen19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
