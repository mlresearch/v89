---
title: Learning Invariant Representations with Kernel Warping
abstract: Invariance is an effective prior that has been extensively used to bias
  supervised learning with a \emph{given} representation of data. In order to learn
  invariant representations,  wavelet and scattering based methods “hard code” invariance
  over the \emph{entire} sample space, hence restricted to a limited range of transformations.
  Kernels based on Haar integration also work only on a \emph{group} of transformations.
  In this work, we break this limitation by designing a new representation learning
  algorithm that incorporates invariances \emph{beyond transformation}. Our approach,
  which is based on warping the kernel in a data-dependent fashion, is computationally
  efficient using random features, and leads to a deep kernel through multiple layers.
  We apply it to convolutional kernel networks and demonstrate its stability.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ma19a
month: 0
tex_title: Learning Invariant Representations with Kernel Warping
firstpage: 1003
lastpage: 1012
page: 1003-1012
order: 1003
cycles: false
bibtex_author: Ma, Yingyi and Ganapathiraman, Vignesh and Zhang, Xinhua
author:
- given: Yingyi
  family: Ma
- given: Vignesh
  family: Ganapathiraman
- given: Xinhua
  family: Zhang
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/ma19a/ma19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/ma19a/ma19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
