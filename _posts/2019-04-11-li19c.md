---
title: On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes
abstract: 'Stochastic gradient descent is the method of choice for large scale optimization
  of machine learning objective functions. Yet, its performance is greatly variable
  and heavily depends on the choice of the stepsizes. This has motivated a large body
  of research on adaptive stepsizes. However, there is currently a gap in our theoretical
  understanding of these methods, especially in the non-convex setting. In this paper,
  we start closing this gap: we theoretically analyze in the convex and non-convex
  settings a generalized version of the AdaGrad stepsizes. We show sufficient conditions
  for these stepsizes to achieve almost sure asymptotic convergence of the gradients
  to zero, proving the first guarantee for generalized AdaGrad stepsizes in the non-convex
  setting. Moreover, we show that these stepsizes allow to automatically adapt to
  the level of noise of the stochastic gradients in both the convex and non-convex
  settings, interpolating between O(1/T) and O(1/sqrt(T)), up to logarithmic terms.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: li19c
month: 0
tex_title: On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes
firstpage: 983
lastpage: 992
page: 983-992
order: 983
cycles: false
bibtex_author: Li, Xiaoyu and Orabona, Francesco
author:
- given: Xiaoyu
  family: Li
- given: Francesco
  family: Orabona
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of Machine Learning Research
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/li19c/li19c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/li19c/li19c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
