---
title: Direct Acceleration of SAGA using Sampled Negative Momentum
abstract: Variance reduction is a simple and effective technique that accelerates
  convex (or non-convex) stochastic optimization. Among existing variance reduction
  methods, SVRG and SAGA adopt unbiased gradient estimators and are the most popular
  variance reduction methods in recent years. Although various accelerated variants
  of SVRG (e.g., Katyusha and Acc-Prox-SVRG) have been proposed, the direct acceleration
  of SAGA still remains unknown. In this paper, we propose a directly accelerated
  variant of SAGA using a novel Sampled Negative Momentum (SSNM), which achieves the
  best known oracle complexity for strongly convex problems (with known strong convexity
  parameter). Consequently, our work fills the void of directly accelerated SAGA.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhou19c
month: 0
tex_title: Direct Acceleration of SAGA using Sampled Negative Momentum
firstpage: 1602
lastpage: 1610
page: 1602-1610
order: 1602
cycles: false
bibtex_author: Zhou, Kaiwen and Ding, Qinghua and Shang, Fanhua and Cheng, James and
  Li, Danli and Luo, Zhi-Quan
author:
- given: Kaiwen
  family: Zhou
- given: Qinghua
  family: Ding
- given: Fanhua
  family: Shang
- given: James
  family: Cheng
- given: Danli
  family: Li
- given: Zhi-Quan
  family: Luo
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of Machine Learning Research
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/zhou19c/zhou19c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/zhou19c/zhou19c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
