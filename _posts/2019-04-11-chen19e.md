---
title: Renyi Differentially Private ERM for Smooth Objectives
abstract: In this paper, we present a Renyi Differentially Private stochastic gradient
  descent (SGD) algorithm for convex empirical risk minimization. The algorithm uses
  output perturbation and leverages randomness inside SGD, which creates a "randomized
  sensitivity", in order to reduce the amount of noise that is added. One of the benefits
  of output perturbation is that we can incorporate a periodic averaging step that  serves
  to further reduce sensitivity while improving accuracy (reducing the well-known
  oscillating behavior of SGD near the optimum).  Renyi Differential Privacy can be
  used to provide (epsilon, delta)-differential privacy guarantees and hence provide
  a comparison with prior work. An empirical evaluation demonstrates that the proposed
  method outperforms prior methods on differentially private ERM.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: chen19e
month: 0
tex_title: Renyi Differentially Private ERM for Smooth Objectives
firstpage: 2037
lastpage: 2046
page: 2037-2046
order: 2037
cycles: false
bibtex_author: Chen, Chen and Lee, Jaewoo and Kifer, Dan
author:
- given: Chen
  family: Chen
- given: Jaewoo
  family: Lee
- given: Dan
  family: Kifer
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/chen19e/chen19e.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/chen19e/chen19e-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
