---
title: Truncated Back-propagation for Bilevel Optimization
abstract: Bilevel optimization has been recently revisited for designing and analyzing
  algorithms in hyperparameter tuning and meta learning tasks. However, due to its
  nested structure, evaluating exact gradients for high-dimensional problems is computationally
  challenging. One heuristic to circumvent this difficulty is to use the approximate
  gradient given by performing truncated back-propagation through the iterative optimization
  procedure that solves the lower-level problem. Although promising empirical performance
  has been reported, its theoretical properties are still unclear. In this paper,
  we analyze the properties of this family of approximate gradients and establish
  sufficient conditions for convergence. We validate this on several hyperparameter
  tuning and meta learning tasks. We find that optimization with the approximate gradient
  computed using few-step back-propagation often performs comparably to optimization
  with the exact gradient, while requiring far less memory and half the computation
  time.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shaban19a
month: 0
tex_title: Truncated Back-propagation for Bilevel Optimization
firstpage: 1723
lastpage: 1732
page: 1723-1732
order: 1723
cycles: false
bibtex_author: Shaban, Amirreza and Cheng, Ching-An and Hatch, Nathan and Boots, Byron
author:
- given: Amirreza
  family: Shaban
- given: Ching-An
  family: Cheng
- given: Nathan
  family: Hatch
- given: Byron
  family: Boots
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of Machine Learning Research
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/shaban19a/shaban19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/shaban19a/shaban19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
