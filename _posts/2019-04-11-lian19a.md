---
title: 'Revisit Batch Normalization: New Understanding and Refinement via Composition
  Optimization'
abstract: 'Batch Normalization (BN) has been used extensively in deep learning to
  achieve faster training process and better resulting models. However, whether BN
  works strongly depends on how the batches are constructed during training, and it
  may not converge to a desired solution if the statistics on the batch are not close
  to the statistics over the whole dataset. In this paper, we try to understand BN
  from an optimization perspective by providing an explicit objective function associated
  with BN. This explicit objective function reveals that: 1) BN, rather than being
  a new optimization algorithm or trick, is creating a different objective function
  instead of the one in our common sense; and 2) why BN may not work well in some
  scenarios.  We then propose a refinement of BN based on the compositional optimization
  technique called Full Normalization (FN) to alleviate the issues of BN when the
  batches are not constructed ideally. The convergence analysis and empirical study
  for FN are also included in this paper.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: lian19a
month: 0
tex_title: 'Revisit Batch Normalization: New Understanding and Refinement via Composition
  Optimization'
firstpage: 3254
lastpage: 3263
page: 3254-3263
order: 3254
cycles: false
bibtex_author: Lian, Xiangru and Liu, Ji
author:
- given: Xiangru
  family: Lian
- given: Ji
  family: Liu
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of Machine Learning Research
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/lian19a/lian19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/lian19a/lian19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
