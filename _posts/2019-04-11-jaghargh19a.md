---
title: 'Consistent Online Optimization: Convex and Submodular'
abstract: Modern online learning algorithms achieve low (sublinear) regret in a variety
  of diverse settings. These algorithms, however, update their solution at every time
  step. While these updates are computationally efficient, the very requirement of
  frequent updates makes the algorithms untenable in some practical applications.
  In this work we develop online learning algorithms that update a sublinear number
  of times. We give a meta algorithm based on non-homogeneous Poisson Processes that
  gives a smooth trade-off between regret and frequency of updates. Empirically, we
  show that in many cases, we can significantly reduce updates at a minimal increase
  in regret.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: jaghargh19a
month: 0
tex_title: 'Consistent Online Optimization: Convex and Submodular'
firstpage: 2241
lastpage: 2250
page: 2241-2250
order: 2241
cycles: false
bibtex_author: Jaghargh, Mohammad Reza Karimi and Krause, Andreas and Lattanzi, Silvio
  and Vassilvtiskii, Sergei
author:
- given: Mohammad Reza Karimi
  family: Jaghargh
- given: Andreas
  family: Krause
- given: Silvio
  family: Lattanzi
- given: Sergei
  family: Vassilvtiskii
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/jaghargh19a/jaghargh19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/jaghargh19a/jaghargh19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
