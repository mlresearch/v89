---
title: Fast Algorithms for Sparse Reduced-Rank Regression
abstract: We consider a reformulation of Reduced-Rank Regression (RRR) and Sparse
  Reduced-Rank Regression (SRRR) as a non-convex non-differentiable function of a
  single of the two matrices usually introduced to parametrize low-rank matrix learning
  problems. We study the behavior of proximal gradient algorithms for the minimization
  of the objective. In particular, based on an analysis of the geometry of the problem,
  we establish that a proximal Polyak-{Ł}ojasiewicz inequality is satisfied in a neighborhood
  of the set of optima under a condition on the regularization parameter. We can consequently
  derive linear convergence rates for the proximal gradient descent with line search
  and for related algorithms in a neighborhood of the optima. Our experiments show
  that our formulation leads to much faster learning algorithms for RRR and especially
  for SRRR.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: dubois19a
month: 0
tex_title: Fast Algorithms for Sparse Reduced-Rank Regression
firstpage: 2415
lastpage: 2424
page: 2415-2424
order: 2415
cycles: false
bibtex_author: Dubois, Benjamin and Delmas, Jean-Fran\c{c}ois and Obozinski, Guillaume
author:
- given: Benjamin
  family: Dubois
- given: Jean-François
  family: Delmas
- given: Guillaume
  family: Obozinski
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/dubois19a/dubois19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v89/dubois19a/dubois19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
