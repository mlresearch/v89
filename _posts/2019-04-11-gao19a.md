---
title: Auto-Encoding Total Correlation Explanation
abstract: Advances in unsupervised learning enable reconstruction and generation of
  samples from complex distributions, but this success is marred by the inscrutability
  of the representations learned. We propose an information-theoretic approach to
  characterizing disentanglement and dependence in representation learning using multivariate
  mutual information, also called total correlation. The principle of Total Cor-relation
  Ex-planation (CorEx) has motivated successful unsupervised learning applications
  across a variety of domains but under some restrictive assumptions. Here we relax
  those restrictions by introducing a flexible variational lower bound to CorEx. Surprisingly,
  we find this lower bound is equivalent to the one in variational autoencoders (VAE)
  under certain conditions. This information-theoretic view of VAE deepens our understanding
  of hierarchical VAE and motivates a new algorithm, AnchorVAE, that makes latent
  codes more interpretable through information maximization and enables generation
  of richer and more realistic samples.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: gao19a
month: 0
tex_title: Auto-Encoding Total Correlation Explanation
firstpage: 1157
lastpage: 1166
page: 1157-1166
order: 1157
cycles: false
bibtex_author: Gao, Shuyang and Brekelmans, Rob and Steeg, Greg Ver and Galstyan,
  Aram
author:
- given: Shuyang
  family: Gao
- given: Rob
  family: Brekelmans
- given: Greg Ver
  family: Steeg
- given: Aram
  family: Galstyan
date: 2019-04-11
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
volume: '89'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 4
  - 11
pdf: http://proceedings.mlr.press/v89/gao19a/gao19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
